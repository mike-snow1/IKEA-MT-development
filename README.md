# IKEA-MT

This is the repository for developing IKEA-MT models, our in-house translation that capture IKEA's unique tone of voice.

Neural Machine Translation (NMT) is a subfield of Artificial Intelligence and Machine Learning that focuses on the development of models that can automatically translate text from one natural language to another. NMT models are based on neural networks, which are a type of machine learning model that are designed to mimic the structure and function of the human brain.

![Neural network](https://miro.medium.com/max/720/1*BbF4o_uKCRKerXpZiJBlpg.webp)

NMT models typically use an encoder-decoder architecture, where the encoder processes the source text and compresses it into a fixed-length representation called a "context vector". The decoder then uses this context vector to generate the target text. The model is trained using a large dataset of parallel text, which consists of sentences or phrases in the source language and their corresponding translations in the target language.

The training process involves feeding the model with the source sentence and corresponding target sentence, the model then learns to predict the target sentence from the source sentence. During inference, the model takes in a source sentence and generates the most likely target sentence.

NMT models have been shown to be very effective at translating text, often producing translations that are more fluent and natural-sounding than those produced by traditional machine translation methods. These models are used in a variety of applications, including online translation services, automated localization of software and content, and assistive technology for non-native speakers.

There are many different types of NMT model architectures available, such as RNN, LSTM, Transformer, and Transformer-XL etc. Each of these architectures have their own advantages and disadvantages, and the choice of architecture depends on the specific use case and the size and quality of the available training data.
