{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b7aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, \\\n",
    "Seq2SeqTrainingArguments, Seq2SeqTrainer, MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c77c70-e0e4-4b84-a1d1-d0cadb933ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "model = \"IKEA-MT-development/Models/en_GB-de_DE/IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8031f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IKEA-MT-development/Data/en_GB-de_DE/cleaned_data.csv', engine='python')\n",
    "\n",
    "df = df[['en_GB', 'de_DE']]\n",
    "df = df[(df['en_GB'].notnull()) & (df['de_DE'].notnull())]\n",
    "\n",
    "df.drop_duplicates()\n",
    "\n",
    "data = Dataset.from_pandas(pd.DataFrame({'translation': df.to_dict('records')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a8c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_valid = data.train_test_split(test_size=0.0015)\n",
    "\n",
    "test_valid = train_test_valid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test_valid['train'],\n",
    "    'validation': test_valid['test'],\n",
    "    'test': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf038dd-34e4-43b9-83ed-74c8326886c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 3887030\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2920\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2920\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13226599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'de_DE': '80 der Hundert größten Unternehmen der Europäischen Union werden durch nur drei Agenturen kontrolliert  . Gerade deshalb sollte das Rating von Unternehmen auch von diesen selbst besser nachvollzogen werden können  .', 'en_GB': 'Of the hundred largest enterprises in the European Union  , 80 are monitored by only three agencies  , which should  , therefore  , be able to do a better job of rating them  .'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'de_DE': 'Schätzungen zufolge wurden 45 000 Soldaten gegen 5 000 Rebellen in die Provinz entsandt  .', 'en_GB': 'An estimated 45 000 troops have been sent into the province against 5000 rebels  .'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'de_DE': 'Die Erfahrungen der Gerichtshöfe  , die wir bereits hatten  , der Kriegsverbrechertribunale für Ruanda und Jugoslawien  , machen deutlich  , wie unzureichend das gegenwärtige System bei der Ahndung solcher von mir genannter Verbrechen ist  .', 'en_GB': 'Experience from those courts we have set up  , namely the war crimes tribunals for Rwanda and Yugoslavia  , clearly show that the present system is inadequate for dealing with the type of crimes I have just mentioned  .'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'de_DE': 'Das RP7 ist zentraler Bestandteil der Lissabon-Strategie für Wachstum und Arbeitsplätze  .', 'en_GB': 'FP7 is a central part of the Lisbon Strategy for growth and jobs  .'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'de_DE': 'Booking  .com: Hotel Holiday Inn Manhattan Sixth Avenue  , Manhattan (New York)  , Vereinigte Staaten - 537 Gästebewertungen  .', 'en_GB': 'Booking  .com: hotel Holiday Inn Manhattan Sixth Avenue  , Manhattan (New York)  , United States of America - 549 Guest reviews  .'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(data[[random.randint(0, len(data)) for i in range(5)]])\n",
    "\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1196e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff7a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(dataset, source_len=128, target_len=128, source=\"en_GB\", target=\"de_DE\"):\n",
    "    \"\"\"\n",
    "    Output: Generates tokenized data using the attributes of the base model\n",
    "    :param dataset: raw dataset to transform\n",
    "    :param source_len: maximum sentence length for source string\n",
    "    :param target_len: maximum sentence length for target string\n",
    "    :param source: source language\n",
    "    :param target: target language\n",
    "    \"\"\"\n",
    "    inputs = [s[source] for s in dataset[\"translation\"]]\n",
    "    targets = [s[target] for s in dataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=source_len, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=target_len, truncation=True)\n",
    "        \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb915ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff746efd14643d39d7875eebf007c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3888 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17694/1636391103.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    780\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m                 )\n\u001b[0;32m--> 782\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             }\n\u001b[1;32m    784\u001b[0m         )\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    780\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m                 )\n\u001b[0;32m--> 782\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             }\n\u001b[1;32m    784\u001b[0m         )\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2362\u001b[0m                 \u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m                 \u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2364\u001b[0;31m                 \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2365\u001b[0m             )\n\u001b[1;32m   2366\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m         }\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2736\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m                                 \u001b[0mcheck_same_num_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2738\u001b[0;31m                                 \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2739\u001b[0m                             )\n\u001b[1;32m   2740\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mNumExamplesMismatchError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2613\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2614\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2615\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2616\u001b[0m                 \u001b[0;31m# Check if the function returns updated examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2304\u001b[0m                 )\n\u001b[1;32m   2305\u001b[0m                 \u001b[0;31m# Use the LazyDict internally, while mapping the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2306\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecorated_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2307\u001b[0m                 \u001b[0;31m# Return a standard dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2308\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17694/2009021703.py\u001b[0m in \u001b[0;36mprocessing\u001b[0;34m(dataset, source_len, target_len, source, target)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_target_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2492\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2494\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2495\u001b[0m             )\n\u001b[1;32m   2496\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m         )\n\u001b[1;32m   2687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_trie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0;31m# [\"This is something\", \"<special_token_1>\", \"  else\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/MT/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;31m# If we look at \"lowball\", we're going to match \"l\" (add it to states), \"o\", \"w\", then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# fail on \"b\", we need to remove 0 from the valid states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mto_remove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;31m# Whenever we found a match, we need to drop everything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;31m# this is a greedy algorithm, it will match on the first found token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(processing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca3c8aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3e50c7c9c1494e838d9a4bda9e9875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/284M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "960ceb7a-430e-4e84-bec4-5ba6f9205dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e02f641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04717ac5f87a4d0790a0a63a3a98c54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(predictions):\n",
    "    \"\"\"\n",
    "    Output: evaluation metrics to track model performance in training\n",
    "    :param predictions: output of predictions to decode\n",
    "    \"\"\"\n",
    "    def process_text(predictions, labels):\n",
    "        preds = [pred.strip() for pred in predictions]\n",
    "        labels = [[label.strip()] for label in labels]\n",
    "        return preds, labels\n",
    "    \n",
    "    preds, labels = predictions\n",
    "    \n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = process_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()} #round results\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c67826a-9fb2-4cef-96c7-542a6a0d3ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "time = str(datetime.now())\n",
    "\n",
    "batch_size = 16\n",
    "model_name = 'IKEA_MT_en-de'\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}_en_GB-to-de_DE_{time}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=20,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8655a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation. If translation are not expected by `MarianMTModel.forward`,  you can safely ignore this message.\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3887030\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 303675\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19182' max='303675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 19182/303675 3:20:08 < 49:28:43, 1.60 it/s, Epoch 0.32/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-500/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-1000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-1000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-1000/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-1500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-1500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-1500/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-2000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-2000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-2000/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-2500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-2500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-2500/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-3000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-3000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-3000/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-3500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-3500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-3500/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-4000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-4000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-4000/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-4500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-4500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-4500/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-5000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-5000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-5000/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-5500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-5500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-5500/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-6000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-6000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-6000/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-6500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-6500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-6500/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-7000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-7000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-7000/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-8000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-8000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-8000/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-8500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-8500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-8500/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-9000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-9000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-9000/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-9500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-9500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-9500/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-10000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-10000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-10000/special_tokens_map.json\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-10500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-10500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-500] due to args.save_total_limit\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-11000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-11000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-1000] due to args.save_total_limit\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-11500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-11500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-1500] due to args.save_total_limit\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-12000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-12000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-2000] due to args.save_total_limit\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-12500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-12500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-2500] due to args.save_total_limit\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-13000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-13000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-3000] due to args.save_total_limit\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-13500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-13500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-3500] due to args.save_total_limit\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-14000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-14000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-4000] due to args.save_total_limit\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-14500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-14500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-4500] due to args.save_total_limit\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-15000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-15000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-5000] due to args.save_total_limit\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-15500\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-15500/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-5500] due to args.save_total_limit\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-16000\n",
      "Configuration saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-16000/config.json\n",
      "Model weights saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [IKEA_MT_en-de_en_GB-to-de_DE_2022-07-29 10:21:41.553928/checkpoint-6000] due to args.save_total_limit\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d46ba1a9-6735-43cc-a210-9a1af18ea684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862\n",
      "Configuration saved in IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/config.json\n",
      "Model weights saved in IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/pytorch_model.bin\n",
      "tokenizer config file saved in IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/tokenizer_config.json\n",
      "Special tokens file saved in IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('IKEA-MT_en-GB_de-DE_' + time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb3cba-7a8d-4676-b020-51421107be18",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7691559f-d8b1-4017-a462-577d9c85eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = \"AO57/58\"\n",
    "model_name = \"../EU_IKEA_clean_data_2022-06-28 00:03:00.355629/\"\n",
    "\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name, output_loading_info=False)\n",
    "\n",
    "def inference(src_text):\n",
    "    \"\"\"\n",
    "    Output: Translated text of the source text\n",
    "    :param src_text: source string\n",
    "    \"\"\"   \n",
    "    translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=False))\n",
    "    translated_txt = [tokenizer.decode(s, skip_special_tokens=True) for s in translated]\n",
    "\n",
    "    return translated_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4d5d08-c65e-4201-804e-f2df8949cdb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Druckbildschirm']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = \"printscreen\"\n",
    "\n",
    "inference(src_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "216dc91a-1683-449d-b0dd-24715e454f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wir haben Euren Vorfallbericht erhalten. Eure Ticketnummer lautet <XXXXXX> (zeigt als Hyperlink zum aktuellen Ticket), ihr müsst im Moment nichts unternehmen – wir melden euch in Kürze.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = \"We have received your incident report. Your ticket number is <XXXXX> (shows as a hyperlink to the actual ticket), You don’t need to do anything at the moment – we will get back to you soon.\"\n",
    "\n",
    "inference(src_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "581e8782-1657-4576-a3b6-266109670b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-de\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      58100\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 58100,\n",
      "  \"decoder_vocab_size\": 58101,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 58100,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 58101\n",
      "}\n",
      "\n",
      "loading weights file IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/target_vocab.json. We won't load it.\n",
      "Didn't find file IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/added_tokens.json. We won't load it.\n",
      "loading file IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/source.spm\n",
      "loading file IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/target.spm\n",
      "loading file IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/vocab.json\n",
      "loading file None\n",
      "loading file IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/tokenizer_config.json\n",
      "loading file None\n",
      "loading file IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "test_set = pd.read_csv('test_sample.csv')\n",
    "# test_set = pd.read_excel('NowIT-stefan.xlsx')\n",
    "# test_set['en_GB'] = test_set['Source text'].apply(lambda s: s.replace('\\t', '').replace('\\n', ''))\n",
    "# test_set['en_GB'] = test_set['en_GB'].apply(lambda s: s.replace(\"\\\\\", ''))\n",
    "# test_set['en_GB'] = test_set['en_GB'].apply(lambda s: s.replace(\".\", ' ').replace(\",\", ' ').replace(\"!\", ' '))\n",
    "\n",
    "model_path = \"IKEA-MT_en-GB_de-DE_2022-08-01 08:36:40.937862/\"\n",
    "model = MarianMTModel.from_pretrained(model_path)\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def inference(src_text):\n",
    "\n",
    "    translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=False))\n",
    "    translated_txt = [tokenizer.decode(s, skip_special_tokens=True) for s in translated]\n",
    "\n",
    "    return translated_txt\n",
    "\n",
    "test_set['Translation'] =  test_set['en_GB'].apply(lambda s: inference(s)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7d16348-577b-47cb-b1fb-eabd9448791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_excel('new_model_v4.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d39a533-3f9c-4f51-8541-093631b307cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['en_GB'] = test_set['Source text'].apply(lambda s: s.replace('\\t', '').replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6c3fa06-7642-4e56-97d4-f5b85bb1d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['en_GB'] = test_set['en_GB'].apply(lambda s: s.replace(\"/\", ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51322da7-f842-4d04-9eed-75d6a9b32f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.Incident  Received (caller and watch list user)'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['en_GB'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "821c36f1-61e2-472a-9910-02eb559ae06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation. If translation are not expected by `MarianMTModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2920\n",
      "  Batch size = 64\n",
      "/home/jupyter/Envs/MT/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46/46 03:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5674965381622314,\n",
       " 'eval_bleu': 27.983,\n",
       " 'eval_gen_len': 33.1442,\n",
       " 'eval_runtime': 245.5699,\n",
       " 'eval_samples_per_second': 11.891,\n",
       " 'eval_steps_per_second': 0.187}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e9b5d-4f6e-4ae0-a86f-1eeecd74b865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "mt",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "mt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
