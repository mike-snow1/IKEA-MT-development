{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b399d1eb-66f9-46cd-965d-c45fca2d39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import display, HTML\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e144d3-0504-4d6f-a3c1-0df0d76ecb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \"\"\"\n",
    "    Output: Translated text of the source text\n",
    "    :param src_text: source string\n",
    "    \"\"\" \n",
    "    \n",
    "    #Load data (needs to be changed to loading from BQ or bucket)\n",
    "    df = pd.read_csv('../../data/en_GB-it_IT/en_GB-it_IT_batch_0.csv', engine='python')\n",
    "\n",
    "    df = df[['source', 'target']] # will be redundent when setup \n",
    "    df['source'] = df['source'].apply(lambda s: str(s))\n",
    "    df['target'] = df['target'].apply(lambda s: str(s))\n",
    "\n",
    "    # Transform to HuggingFace Dataset\n",
    "    data = Dataset.from_pandas(pd.DataFrame({'translation': df.to_dict('records')})) \n",
    "\n",
    "    # Split data into training sets\n",
    "    train_test_valid = data.train_test_split(shuffle=True, seed=7, test_size=0.0015)\n",
    "    test_valid = train_test_valid['test'].train_test_split(shuffle=True, seed=7, test_size=0.5)\n",
    "\n",
    "    # Convert to train/validate/test\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_test_valid['train'],\n",
    "        'validation': test_valid['test'],\n",
    "        'test': test_valid['train']})\n",
    "     \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989f5644-1dcb-4147-a0b7-31f4266e3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_processing(dataset, source_len=128, target_len=128,\n",
    "                            source=\"source\", target=\"target\"):\n",
    "    \"\"\"\n",
    "    Output: Generates tokenized data using the attributes of the base model\n",
    "    :param dataset: raw dataset to transform\n",
    "    :param source_len: maximum sentence length for source string\n",
    "    :param target_len: maximum sentence length for target string\n",
    "    :param source: source language\n",
    "    :param target: target language\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = [s[source] for s in dataset[\"translation\"]]\n",
    "    targets = [s[target] for s in dataset[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=source_len, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=target_len, truncation=True)\n",
    "        \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4f5deb-b075-4464-ab6b-547bfdcdea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions): # n.b. can add more metrics later\n",
    "    \"\"\"\n",
    "    Output: evaluation metrics to track model performance in training\n",
    "    :param predictions: output of predictions to decode\n",
    "    \"\"\"\n",
    "    \n",
    "    metric = load_metric(\"sacrebleu\")\n",
    "    \n",
    "    def process_text(predictions, labels):\n",
    "        preds = [pred.strip() for pred in predictions]\n",
    "        labels = [[label.strip()] for label in labels]\n",
    "        return preds, labels\n",
    "    \n",
    "    preds, labels = predictions\n",
    "    \n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = process_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    \n",
    "    result = {k: round(v, 6) for k, v in result.items()} #round results\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd5d117-6c02-48da-8a62-19d55ff7b0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load models and data collator\n"
     ]
    }
   ],
   "source": [
    "model_name=\"Helsinki-NLP/opus-mt-en-it\"\n",
    "\n",
    "dataset = process_data()\n",
    "\n",
    "print('load models and data collator')\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722a6fba-3f0a-41e6-8a79-a635cdc50c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a528b3fd4e84589b817e1c0bf6e4de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e28636b6b249b283f92eee4a9adeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2868bcd476494aa08f5075fb664f7b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenization_processing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d87817cc-2a72-4ed3-b8ec-913c2a762825",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = str(datetime.now())\n",
    "\n",
    "batch_size=16\n",
    "learning_rate=2e-5 \n",
    "weight_decay=0.01\n",
    "save_limit=10\n",
    "epochs=20\n",
    "\n",
    "# n.b. need to add early stopping with longer training times\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"../../models/checkpoints/IKEA_MT_en_GB-it_IT_{time}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay = weight_decay,\n",
    "    save_total_limit = save_limit,\n",
    "    num_train_epochs = epochs,\n",
    "    predict_with_generate=True    \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset = tokenized_dataset[\"train\"],\n",
    "    eval_dataset = tokenized_dataset[\"validation\"],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1620c567-83c4-41ac-a9b8-f0eb1ee58933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation. If translation are not expected by `MarianMTModel.forward`,  you can safely ignore this message.\n",
      "/home/jupyter/.virtualenvs/ikea-mt/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 199112\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 248900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8890' max='248900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  8890/248900 30:08 < 13:33:49, 4.92 it/s, Epoch 0.71/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-500\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-500/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-1000\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-1000/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-1500\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-1500/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-2000\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-2000/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-2500\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-2500/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-3000\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-3000/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-3500\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-3500/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-4000\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-4000/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-4500\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-4500/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-5000\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-5000/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-5500\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-5500/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-6000\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-6000/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-6500\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-6500/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-7000\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-7000/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-7500\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-7500/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-8000\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-8000/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-8500\n",
      "Configuration saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-8500/config.json\n",
      "Model weights saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [../../models/checkpoints/IKEA_MT_en_GB-it_IT_2022-12-23 14:07:24.526177/checkpoint-3500] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "print('train model')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d123cc7-4470-4e6a-8f92-f52760d932ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('save model')\n",
    "trainer.save_model('../../models/en_GB-it_IT/IKEA_MT-en_it_IT_' + time)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "ikea-mt",
   "name": "pytorch-gpu.1-12.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m100"
  },
  "kernelspec": {
   "display_name": "ikea-mt",
   "language": "python",
   "name": "ikea-mt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
